{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo os autores da ferramenta, XGBboost (**Extreme Gradient Boosting**) é um sistema de _tree bosting_ escalável de ponta a ponta, ou, em suas palavras:\n",
    "\n",
    "> \"a scalable end-to-end tree boosting system called XGBoost\"\n",
    "\n",
    "Certo é um sistema baseada em _tree boosting_. Porém, qual o significado do termo \"**boosting**\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo **Zhi-Hua Zhou p. 23**, livro *Ensemble Methods: Foundations and Algorithms*:\n",
    "> o termo _Boosting_ se refere a uma família de algoritmos capazes de converter _weak learners_ em _strong learners_\n",
    "\n",
    "Certo, mas o que seriam _weak learners_ e *strong learners*?  \n",
    "De maneira simplificada \\[e intuitiva\\]:\n",
    "\n",
    "**1. Definição: um _weak learner_ é um modelo com desempenho ligeiramente superior ao método aleatório (advinhação aleatória)**\n",
    "\n",
    "Certo, e quanto a um *strong learner*?\n",
    "Similarmente:\n",
    "\n",
    "**2. Definição: um _strong learner_ é um modelo muito próximo à perfeição**\n",
    "\n",
    "A partir desses dois conceitos, surge a ideia de _converter_ **weak learner** &rarr; **strong learner**. A partir desse conceito, provou-se \\[por contrução\\] a equivalência entre essas classes. E a tal contrução foi atribuído o nome _**boosting**_.  \n",
    "\n",
    "A título de exemplo, temos a técnica *gradient boosting* a qual consiste em, dado um *comitê* composto por um modelo, suas predições referentes ao um conjunto de dados e os rótulos corretos, adicionar ao comitê um novo modelo o qual *corrija* os erros do classificador passado. Esse processo de inclusão de novos modelos é repetido até que algum critério de parada seja atendido.\n",
    "\n",
    "<!-- [intro-boosting]: https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Obs: para os fins dessa explicação, considere tratar-se de um problema de classificação. A definição pode ser trivialmente estendida para problemas de regressão.\n",
    "\n",
    "A ideia do referido **gradient boosting** é a construção de um **_strong learner_** a partir da _adição_ de novos classificadores a partir de um classificador base. Cada novo classificador deve reduzir os _erros cometidos pelo classificador anterior_ (grosso modo, \"compensá-los\"). Desse modo, cada novo classificador adicionado tem por objetivo corrigir as predições do antigo classificador. Esse procedimento continua até que se atinja um critério de parada.\n",
    "\n",
    "> Nota: é possível a ocorrência de _overfitting_ caso o procedimento ocorra demasiadas vezes.\n",
    "\n",
    "Ou seja: **_gradient boosting_** tem por objetivo a contrução de uma sequência de _comitês_ de classificadores (usualmente, árvores de decisão) cujo objetivo é ser cada vez melhor que o comitê anterior.  \n",
    "\n",
    "Há pesquisas acerca da utilização de [redes neurais \"rasas\" enquanto _weak learners_][nn-weak-learner].\n",
    "\n",
    "[nn-weak-learner]: https://arxiv.org/pdf/2002.07971.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumidamente, **XGBoost** é um framework que implementa **gradient boosting**, porém com um foco em _performance e escalabilidade_.  \n",
    "\n",
    "Por exemplo, o sistema é _\"sparsity awareness\"_ (ou seja, a esparsidade dos dados é levada em consideração pelo sistema a fim de melhor gerenciar os recursos computacionais disponíveis) garantindo assim uma maior escalabilidade. \n",
    "Além disso, o modo de funcionamento da cache (pg.2 artigo XGBoost) é levado em consideração para construção de estruturas de dados possibilitando ganho de performance em até 2x-3x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visão geral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost é não apenas um algoritmo, mas um *sistema completo* baseada no algoritmo **gradient boosting**. O funcionamento deste consiste em, dado um modelo inicial _fraco_ (weak learner), ir adicionando novos modelos ao comitê esperando pela correção dos erros passados através dessas novas adições. A cada iteração do método, um novo modelo (nesse caso, uma *árvore de regressão*) é adicionada ao comitê e a previsão dele é dada por\n",
    "\n",
    "\\begin{equation*}\n",
    "\\widehat{y_1}=\\phi(x_i)=\\sum_{k=1}^Kf_k(x_i),f_k\\in \\boldsymbol{\\mathcal{F}}\n",
    "\\label{eq1} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Uma vez especificado como é realizada a predição, é necessário definir alguma função objetivo para minimização. Tal função é\n",
    "\n",
    "\\begin{equation*}\n",
    "\\boldsymbol{\\mathfrak{L}}^{(t)}=\\sum_{i=1}^nl(y_i,\\widehat{y}_i^{(t-1)}+f_t(x_i))+\\Omega(f_t))\n",
    "\\label{eq2} \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "`l` é uma **função de perda** *convexa* (para que a aplicação do gradienta faça sentido), `y` é o valor real, `ŷ` é a previsão do comitê e  `Ômega` é um fator de *regularização* que `penaliza a última árvore adicionada ao comitê  por sua complexidade`.  \n",
    "\n",
    "A estratégia é escolher uma função\n",
    "\\begin{equation*}\n",
    "f_i(x_i)\n",
    "\\label{eq3} \\tag{3}\n",
    "\\end{equation*}\n",
    "que minimize L.\n",
    "\n",
    "Após uma série de operações matemáticas incluindo séries de Taylor para aproximação de `l` e computação de pesos ótimo para cada árvore `f`, obtem-se um espécie de *medida de impureza* para as árvores de decisão (obs: `g` e `h` são os gradientes de 1ª e 2ª ordem da função de perda `l`). Essa medida,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\boldsymbol{\\mathfrak{L}}_{split}=\\frac{1}{2} \\left [\\frac{(\\sum_{i\\in I_L }g_i)^2)}{\\sum_{i\\in I_L}h_i+\\lambda} +\\frac{(\\sum_{i\\in I_R}g_i)^2}{\\sum_{i\\in I_R}h_i+\\lambda} - \\frac{(\\sum_{i\\in I}g_i)^2}{\\sum_{i\\in I}h_i+\\lambda} \\right ]-\\gamma\n",
    "\\label{eq4} \\tag{4}\n",
    "\\end{equation*}\n",
    "\n",
    "costuma ser utilizada para decidir entre os candidatos para divisão da árvore (é usada no algoritmo de divisão aproximado e exato).  \n",
    "\n",
    "\n",
    "\n",
    "## Divisão da árvore\n",
    "\n",
    "Sendo impossível a computação de todas as estruturas de árvores possíveis em tempo razoável, **XGBoost** adota um algoritmo *guloso* para gerar cada árvore. Conforme descrito em seu artigo, existe um \"algoritmo guloso exato\" para encontrar a estrutura ótima; porém, conforme citado previamente, é computacionalmente inviável fazê-lo caso os dados não caibam em memória principal. Nesses casos, é utilizado um algoritmo *guloso aproximado*.\n",
    "\n",
    "### Algoritmo exato\n",
    "Para cada novo nó da árvore, computa todos os possíveis ganhos para todas as divisões possíveis antes de realizar uma divisão, sempre realizando a divisão que trará o maior *score* (como um algoritmo de árvore de decisão genérico). A peculiaridade aqui fica por conta da utilização da fórmula em \\eqref{eq4} utilizada durante essa avaliação.\n",
    "\n",
    "\n",
    "### Algoritmo aproximado\n",
    "Resumidamente, conforme seção 3.2 do artigo original, o algoritmo funciona com as seguintes etapas:\n",
    "\n",
    "1. baseado em percentis das *features*, propõe valores a serem usados para divisão da árvores;\n",
    "2. discretização (contínuo &rarr; intervalos); fazer o mesmo procedimento descrito no item anterior\n",
    "3. realizar os passos do `algoritmo exato`, com a diferença de que o espaço de busca foi reduzido\n",
    "\n",
    "\n",
    "### \"Sparsity aware\"\n",
    "A forma como é tratada a esparsidade dos dados (valores ausentes, variáveis com muitos zeros, etc) deve fazer parte do algoritmo. Isso é feito por meio de *decisões padrão* em cada nó da árvore. Cada \"decisão-padrão\" é aprendida a partir dos dados.\n",
    "\n",
    "<img src=\"img/default-decision.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outras otimizações\n",
    "\n",
    "Além do algoritmo descrito, diversas otimizações são implementadas.  \n",
    "As mais importantes são:\n",
    "\n",
    "#### Pré-ordenamento das colunas em uma estrutura referida no artigo como `bloco`\n",
    "Cada bloco armazenado no formato **CSC** (`compressed columns`, uma forma compacta de armazenar dados utilizada também na biblioteca `scipy`).\n",
    "A principal contribuição dessa abordagem é não ter que ordenar os dados inúmeras vezes ao longo do processo de construção das árvores.\n",
    "Além disso, o fato dos dados serem divididos em blocos permite por exemplo sua distribuição entre máquinas, permitindo assim paralelização do acessos aos dados (organizados por blocos em colunas).\n",
    "#### Acesso ciente da cache\n",
    "Permite ganho de 2-3x de performance para grandes conjuntos de dados.\n",
    "\n",
    "#### computação \"out-of-core\"\n",
    "Para conjuntos de dados grandes o bastante que não cabem na memória principal, XGBoost utiliza a estratégia de *salvar em disco* os `blocos` de dados em conjunto com a utilização de 1 *thread* independente para carregar esses blocos em memória conforme se faça necessário.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "0. [Artigo do xgboost][xgboost]\n",
    "1. [Ensemble methods; Zhi-Hua][ensemble-methods-bib]\n",
    "2. [Compressed Sparse Column Format (CSC)][csc]\n",
    "3. [CSC - wikipedia][csc-wiki]\n",
    "4. [Função de perda][loss-fun]\n",
    "5. [Função convexa][conv-fun]\n",
    "6. [Árvores de decisão][dec-tree]\n",
    "7. [Séries de Taylor][taylor]\n",
    "8. [Guloso][greedy]\n",
    "9. [*gradient boosting*][grad-boost]\n",
    "10. [Ensemble methods][ens-meth]\n",
    "11. [Ensemble learning][ens-learn]\n",
    "\n",
    "[xgboost]: https://arxiv.org/pdf/1603.02754.pdf\n",
    "[ensemble-methods-bib]: https://dl.acm.org/doi/book/10.5555/2381019\n",
    "[csc]: https://scipy-lectures.org/advanced/scipy_sparse/csc_matrix.html\n",
    "[csc-wiki]: https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS)\n",
    "[conv-fun]: https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_convexa\n",
    "[loss-fun]: https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_de_perda\n",
    "[dec-tree]: https://medium.com/@gabriel.stankevix/arvore-de-decis%C3%A3o-em-r-85a449b296b2\n",
    "[taylor]: https://pt.wikipedia.org/wiki/S%C3%A9rie_de_Taylor\n",
    "[greedy]: https://www.ime.usp.br/~pf/analise_de_algoritmos/aulas/guloso.html\n",
    "[grad-boost]: https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n",
    "[ens-meth]: https://www.amazon.com/Ensemble-Methods-Foundations-Algorithms-Recognition/dp/1439830037\n",
    "[ens-learn]: http://www.scholarpedia.org/article/Ensemble_learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
